{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemantCopilot/g_test/blob/dev/Stable_Diffusion_with_SwinIR_plus_Real_ESRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-vX3cavOCt"
      },
      "source": [
        "# **Stable Diffusion** 🎨 \n",
        "*...using `🧨diffusers`*\n",
        "\n",
        "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It's trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM.\n",
        "See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information.\n",
        "\n",
        "This Colab notebook shows how to use Stable Diffusion with the 🤗 Hugging Face [🧨 Diffusers library](https://github.com/huggingface/diffusers). \n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlvQ1nQL7c"
      },
      "source": [
        "### Setup\n",
        "\n",
        "First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster. If the following command fails, use the `Runtime` menu above and select `Change runtime type`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHkHsdtnry57"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJt_cx5QgVz"
      },
      "source": [
        "Next, you should install `diffusers==0.3.0` as well `scipy`, `ftfy` and `transformers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.3.0\n",
        "!pip install transformers scipy ftfy\n",
        "!pip install \"ipywidgets>=7,<8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fcyyt0daU4e"
      },
      "source": [
        "You also need to accept the model license before downloading or using the weights. In this post we'll use model version `v1-4`, so you'll need to  visit [its card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree. \n",
        "\n",
        "You have to be a registered user in 🤗 Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou0Ijygormum"
      },
      "source": [
        "As google colab has disabled external widgtes, we need to enable it explicitly. Run the following cell to be able to use `notebook_login`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtrOo8YPoM2b"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHiV7acka4EY"
      },
      "source": [
        "Now you can login with your user token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TRAh8G6sNfA"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# 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\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NnPOMAqAABv"
      },
      "source": [
        "### Stable Diffusion Pipeline\n",
        "\n",
        "`StableDiffusionPipeline` is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code.\n",
        "\n",
        "First, we load the pre-trained weights of all components of the model.\n",
        "\n",
        "In addition to the model id [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4), we're also passing a specific `revision`, `torch_dtype` and `use_auth_token` to the `from_pretrained` method.\n",
        "`use_auth_token` is necessary to verify that you have indeed accepted the model's license.\n",
        "\n",
        "We want to ensure that every free Google Colab can run Stable Diffusion, hence we're loading the weights from the half-precision branch [`fp16`](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16) and also tell `diffusers` to expect the weights in float16 precision by passing `torch_dtype=torch.float16`.\n",
        "\n",
        "If you want to ensure the highest possible precision, please make sure to remove `revision=\"fp16\"` and `torch_dtype=torch.float16` at the cost of a higher memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:100\"\n",
        "import torch\n",
        "import numpy as np\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "from diffusers import StableDiffusionPipeline,StableDiffusionImg2ImgPipeline\n",
        "\n",
        "# make sure you're logged in with `huggingface-cli login`\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True)  \n",
        "# img2imgpipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",revision=\"fp16\", torch_dtype=torch.float16,use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldl84rPaKCrb"
      },
      "outputs": [],
      "source": [
        "\n",
        "!git clone https://github.com/TencentARC/GFPGAN.git\n",
        "os.chdir(\"/content/GFPGAN\")\n",
        "\n",
        "\n",
        "!pip install basicsr\n",
        "\n",
        "!pip install facexlib\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "#realesr\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "# !pip install realesrgan\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P /content/Real-ESRGAN/experiments/pretrained_models\n",
        "os.chdir(\"/content/Real-ESRGAN\")\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!python setup.py install\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P /content/GFPGAN/experiments/pretrained_models\n",
        "!git clone https://github.com/JingyunLiang/SwinIR.git\n",
        "!pip install timm\n",
        "!wget https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth -P experiments/pretrained_models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/Real-ESRGAN\")\n",
        "from realesrgan import RealESRGANer\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "esrdevice='cuda'\n",
        "with torch.no_grad():\n",
        "  RealESRUpScale = RealESRGANer(model_path=\"/content/Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth\",scale=4,device=esrdevice,model= RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4))\n",
        "os.chdir(\"/content/\")"
      ],
      "metadata": {
        "id": "_DM2JSo6v-1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MgNzTxwbASv"
      },
      "source": [
        "Next, let's move the pipeline to GPU to have faster inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA9myHTxbDhm"
      },
      "outputs": [],
      "source": [
        "pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVC-aLwl8BBX"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "upload_folder = 'upload'\n",
        "result_folder = 'results'\n",
        "final_folder = 'final'\n",
        "version_folder = 'version'\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "if os.path.isdir(final_folder):\n",
        "    shutil.rmtree(final_folder)\n",
        "if os.path.isdir(version_folder):\n",
        "    shutil.rmtree(version_folder)\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)\n",
        "os.mkdir(final_folder)\n",
        "os.mkdir(version_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcgsflpBoEM"
      },
      "source": [
        "\n",
        "\n",
        "Let's first write a helper function to display a grid of images. Just run the following cell to create the `image_grid` function, or disclose the code if you are interested in how it's done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REF_yuHprSa1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcHccTDWbQRU"
      },
      "source": [
        "# Now, we can generate a grid image once having run the pipeline with a list of 4 prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YAFLvWWrSdM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch import autocast\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "num_images = 3\n",
        "prompt = [\"nature landscape for wallpaper, colourfull trees, hd, 4k, cinematic\"] * num_images\n",
        "rn = random.randint(0,10000000)\n",
        "\n",
        "# rn=1624589\n",
        "\n",
        "print(rn)\n",
        "with torch.no_grad():\n",
        "  generator = torch.Generator(\"cuda\").manual_seed(rn)\n",
        "  with autocast(\"cuda\"):\n",
        "    images = pipe(prompt,generator=generator, num_inference_steps=50).images\n",
        "    for i,j in enumerate(images):\n",
        "      j.save(\"./upload/img{}-{}\".format(i,rn)+'.png')\n",
        "images_2up = '/content/upload'\n",
        "torch.cuda.empty_cache()\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "grid\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refine Same Images(Generated in Prev step) with 100 steps. You Can Directly jump to the next Cell"
      ],
      "metadata": {
        "id": "SJk5KfEdDBYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "generator = torch.Generator(\"cuda\").manual_seed(rn)\n",
        "\n",
        "dir = '/content/upload'\n",
        "if os.path.isdir(dir):\n",
        "  for f in os.listdir(dir):\n",
        "     os.remove(os.path.join(dir, f))\n",
        "\n",
        "with autocast(\"cuda\"):\n",
        "  images = pipe(prompt,generator=generator, num_inference_steps=100).images\n",
        "  for i,j in enumerate(images):\n",
        "    j.save(\"./upload/img{}\".format(i)+'.png')\n",
        "images_2up = '/content/upload'\n",
        "torch.cuda.empty_cache()\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "grid\n",
        "\n"
      ],
      "metadata": {
        "id": "jBX2VVz9BeJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refine Same Images(Generated in Prev step) with 200 steps."
      ],
      "metadata": {
        "id": "Ch8UY6Q_DO2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "generator = torch.Generator(\"cuda\").manual_seed(rn)\n",
        "\n",
        "dir = '/content/upload'\n",
        "if os.path.isdir(dir):\n",
        "  for f in os.listdir(dir):\n",
        "     os.remove(os.path.join(dir, f))\n",
        "\n",
        "\n",
        "with autocast(\"cuda\"):\n",
        "  images = pipe(prompt,generator=generator, num_inference_steps=200).images\n",
        "  for i,j in enumerate(images):\n",
        "    j.save(\"./upload/img{}\".format(i)+'.png')\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "images_2up = '/content/upload'\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "grid\n"
      ],
      "metadata": {
        "id": "w1SE6qi5BtxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfnEYcfAQwgn"
      },
      "source": [
        "# **Run This Cell Only if Face is not Restored Completely.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiCQvTuxQEWu"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "# os.chdir(\"/content/GFPGAN\")\n",
        "dir = '/content/results/restored_imgs'\n",
        "if os.path.isdir(dir):\n",
        "  for f in os.listdir(dir):\n",
        "     os.remove(os.path.join(dir, f))\n",
        "with torch.no_grad():     \n",
        "  !python GFPGAN/inference_gfpgan.py -i /content/upload -o /content/results -s 2\n",
        "# os.chdir(\"/content\")\n",
        "\n",
        "\n",
        "def displayGFP(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1) \n",
        "  plt.title('Input image', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('GFPGAN_FaceRestore', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "# display each image in the upload folder\n",
        "\n",
        "images_2up = '/content/results/restored_imgs'\n",
        "\n",
        "input_folder = '/content/upload'\n",
        "result_folder = '/content/results/restored_imgs'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*.png')))\n",
        "output_list = sorted(glob.glob(os.path.join(result_folder, '*.png')))\n",
        "for input_path, output_path in zip(input_list, output_list):\n",
        "  img_input = imread(input_path)\n",
        "  img_output = imread(output_path)\n",
        "  displayGFP(img_input, img_output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Real-ESRGAN Upscale**"
      ],
      "metadata": {
        "id": "5zQLbDEV84wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "imgs_path = sorted(glob.glob(os.path.join(images_2up, '*.png')))\n",
        "imgs = []\n",
        "for path in imgs_path:\n",
        "  imgs.append(Image.open(path).convert('RGB'))\n",
        "\n",
        "\n",
        "def displayESRGAN(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1) \n",
        "  plt.title('Input image', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('ESRGAN', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "import cv2\n",
        "for i,j in enumerate(imgs):\n",
        "  with torch.no_grad():\n",
        "    output, _ = RealESRUpScale.enhance(np.array(j), outscale=4)\n",
        "  output = Image.fromarray(output)\n",
        "  output.save(\"final/fin_img_upscaled_{}.png\".format(i))\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "input_folder = '/content/upload'\n",
        "result_folder = '/content/final'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*.png')))\n",
        "output_list = sorted(glob.glob(os.path.join(result_folder, '*.png')))\n",
        "for input_path, output_path in zip(input_list, output_list):\n",
        "  img_input = imread(input_path)\n",
        "  img_output = imread(output_path)\n",
        "  displayESRGAN(img_input, img_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sme9aQnIJiC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(Optional) Denoise Using SwinIR and Upscale**"
      ],
      "metadata": {
        "id": "Reiik2GxFfRP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDZSZ66VW1cr"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "# os.chdir(\"/content/SwinIR\")\n",
        "torch.cuda.empty_cache()\n",
        "dir = '/content/results/swinir_real_sr_x4_large'\n",
        "if os.path.isdir(dir):\n",
        "  for f in os.listdir(dir):\n",
        "     os.remove(os.path.join(dir, f))\n",
        "\n",
        "if '/content/results/restored_imgs' in images_2up:\n",
        "  !python SwinIR/main_test_swinir.py --task real_sr --model_path /content/experiments/pretrained_models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth --folder_lq results/restored_imgs --scale 4\n",
        "else:\n",
        "  !python SwinIR/main_test_swinir.py --task real_sr --model_path /content/experiments/pretrained_models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth --folder_lq upload --scale 4\n",
        "  \n",
        "\n",
        "\n",
        "def displaySwinIR(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1) \n",
        "  plt.title('Input image', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('SwinIR', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "# display each image in the upload folder\n",
        "images_2up = 'content/results/swinir_real_sr_x4_large'\n",
        "\n",
        "input_folder = '/content/upload'\n",
        "result_folder = '/content/results/swinir_real_sr_x4'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*.png')))\n",
        "output_list = sorted(glob.glob(os.path.join(result_folder, '*.png')))\n",
        "for input_path, output_path in zip(input_list, output_list):\n",
        "  img_input = imread(input_path)\n",
        "  img_output = imread(output_path)\n",
        "  displaySwinIR(img_input, img_output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(Optional) Compare ALL Generated images**"
      ],
      "metadata": {
        "id": "esIeu0OXIPJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "def compare_all(img_input,img_swin,img_esr):\n",
        "  fig,axs = plt.subplots(len(img_input),3,figsize=(35,35))\n",
        "  axs[0,0].set_title(\"Input\", fontsize=16)\n",
        "  axs[0,1].set_title(\"Swin\", fontsize=16)\n",
        "  axs[0,2].set_title(\"ESRGAN\", fontsize=16)\n",
        "  for i in range(len(img_input)):\n",
        "    axs[i,0].imshow(imread(img_input[i]))\n",
        "    axs[i,1].imshow(imread(img_swin[i]))\n",
        "    axs[i,2].imshow(imread(img_esr[i]))\n",
        "  for x in axs.flatten():\n",
        "    x.axis(\"off\")\n",
        "\n",
        "input_folder = '/content/upload'\n",
        "swin_folder = '/content/results/swinir_real_sr_x4'\n",
        "esr_folder = '/content/final'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*.png')))\n",
        "swin_list = sorted(glob.glob(os.path.join(swin_folder, '*.png')))\n",
        "esr_list = sorted(glob.glob(os.path.join(esr_folder, '*.png')))\n",
        "print(len(input_list))\n",
        "print(len(swin_list))\n",
        "print(len(esr_list))\n",
        "\n",
        "compare_all(input_list,swin_list,esr_list)"
      ],
      "metadata": {
        "id": "WiHUZ9QAGr0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW14FA-tDQ5n"
      },
      "source": [
        "## 2. What is Stable Diffusion\n",
        "\n",
        "Now, let's go into the theoretical part of Stable Diffusion 👩‍🎓.\n",
        "\n",
        "Stable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHj_sllMaKTD"
      },
      "source": [
        "General diffusion models are machine learning systems that are trained to *denoise* random gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb).\n",
        "\n",
        "Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsdAj9pDPOv"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Latent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional _latent_ space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: **in latent diffusion the model is trained to generate latent (compressed) representations of the images.** \n",
        "\n",
        "There are three main components in latent diffusion.\n",
        "\n",
        "1. An autoencoder (VAE).\n",
        "2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n",
        "3. A text-encoder, *e.g.* [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4leRMZzjTsA"
      },
      "source": [
        "**1. The autoencoder (VAE)**\n",
        "\n",
        "The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the *U-Net* model.\n",
        "The decoder, conversely, transforms the latent representation back into an image.\n",
        "\n",
        " During latent diffusion _training_, the encoder is used to get the latent representations (_latents_) of the images for the forward diffusion process, which applies more and more noise at each step. During _inference_, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. As we will see during inference we **only need the VAE decoder**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5ZCb66kmyE"
      },
      "source": [
        "**2. The U-Net**\n",
        "\n",
        "The U-Net has an encoder part and a decoder part both comprised of ResNet blocks.\n",
        "The encoder compresses an image representation into a lower resolution image representation and the decoder decodes the lower resolution image representation back to the original higher resolution image representation that is supposedly less noisy.\n",
        "More specifically, the U-Net output predicts the noise residual which can be used to compute the predicted denoised image representation.\n",
        "\n",
        "To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.\n",
        "Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers. The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE7hhg5ArUu4"
      },
      "source": [
        "**3. The Text-encoder**\n",
        "\n",
        "The text-encoder is responsible for transforming the input prompt, *e.g.* \"An astronout riding a horse\" into an embedding space that can be understood by the U-Net. It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings.\n",
        "\n",
        "Inspired by [Imagen](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during training and simply uses an CLIP's already trained text encoder, [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XnKTVfj2Jm"
      },
      "source": [
        "**Why is latent diffusion fast and efficient?**\n",
        "\n",
        "Since the U-Net of latent diffusion models operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models. For example, the autoencoder used in Stable Diffusion has a reduction factor of 8. This means that an image of shape `(3, 512, 512)` becomes `(3, 64, 64)` in latent space, which requires `8 × 8 = 64` times less memory.\n",
        "\n",
        "This is why it's possible to generate `512 × 512` images so quickly, even on 16GB Colab GPUs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz5Ge_47jUaA"
      },
      "source": [
        "**Stable Diffusion during inference**\n",
        "\n",
        "Putting it all together, let's now take a closer look at how the model works in inference by illustrating the logical flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUBqX1sMsDR6"
      },
      "source": [
        "<p align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size $64 \\times 64$ where as the text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder.\n",
        "\n",
        "Next the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, we recommend using one of:\n",
        "\n",
        "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py) (used by default)\n",
        "- [DDIM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)\n",
        "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py)\n",
        "\n",
        "Theory on how the scheduler algorithm function is out of scope for this notebook, but in short one should remember that they compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
        "For more information, we recommend looking into [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)\n",
        "\n",
        "The *denoising* process is repeated *ca.* 50 times to step-by-step retrieve better latent image representations.\n",
        "Once complete, the latent image representation is decoded by the decoder part of the variational auto encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rR2Udg5IYjn"
      },
      "source": [
        "\n",
        "\n",
        "After this brief introduction to Latent and Stable Diffusion, let's see how to make advanced use of 🤗 Hugging Face Diffusers!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}